{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f42b559",
   "metadata": {},
   "source": [
    "# MNIST Classification using RBF-KAN (Radial Basis Function-based Kolmogorov-Arnold Networks)\n",
    "\n",
    "This notebook demonstrates the implementation and training of a RBF-KAN model for the MNIST handwritten digit classification task. The experiment uses the FasterKAN implementation from the `kan_utils` module.\n",
    "\n",
    "## Overview\n",
    "\n",
    "1. **Data Preparation**: Load and preprocess the MNIST dataset\n",
    "2. **Model Setup**: Initialize a FasterKAN model with appropriate architecture\n",
    "3. **Training**: Train the model with validation\n",
    "4. **Evaluation**: Evaluate the model on test data\n",
    "5. **Visualization**: Plot training metrics and model predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0984f27",
   "metadata": {},
   "source": [
    "# Settings\n",
    "\n",
    "Before running this notebook, you can adjust the following settings to customize the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e9c8991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST dataset will be downloaded to: C:\\Users\\mrlnp\\OneDrive - National and Kapodistrian University of Athens\\Υπολογιστής\\KANs\\mnist\\MNIST\\Dataset\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Settings for dataset download\n",
    "DATASET_ROOT = './data'  # Default location to download the MNIST dataset\n",
    "# Alternatively, you can use a custom path like: \n",
    "# DATASET_ROOT = os.path.join(os.path.expanduser('~'), 'datasets', 'mnist')\n",
    "DATASET_ROOT = r\"C:\\Users\\mrlnp\\OneDrive - National and Kapodistrian University of Athens\\Υπολογιστής\\KANs\\mnist\\MNIST\\Dataset\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(DATASET_ROOT, exist_ok=True)\n",
    "\n",
    "# Check if MNIST dataset already exists in the default location\n",
    "mnist_train_path = os.path.join(DATASET_ROOT, 'MNIST', 'raw', 'train-images-idx3-ubyte')\n",
    "mnist_test_path = os.path.join(DATASET_ROOT, 'MNIST', 'raw', 'test-images-idx3-ubyte')\n",
    "\n",
    "if os.path.exists(mnist_train_path) and os.path.exists(mnist_test_path):\n",
    "    print(f\"MNIST dataset already exists at: {os.path.abspath(os.path.join(DATASET_ROOT, 'MNIST'))}\")\n",
    "    print(f\"   Train images: {os.path.getsize(mnist_train_path) / (1024*1024):.2f} MB\")\n",
    "    print(f\"   Test images: {os.path.getsize(mnist_test_path) / (1024*1024):.2f} MB\")\n",
    "else:\n",
    "    print(f\"MNIST dataset will be downloaded to: {os.path.abspath(DATASET_ROOT)}\")\n",
    "\n",
    "# Other global settings\n",
    "USE_CUDA = torch.cuda.is_available()  # Automatically use CUDA if available\n",
    "RANDOM_SEED = 42  # Random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efae8c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Ensure the parent directory is in the path to import kan_utils\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "    print(f\"Added {parent_dir} to sys.path\")\n",
    "\n",
    "# After fixing local imports to use relative imports, we need to import from the package\n",
    "try:\n",
    "    # Import KAN utilities\n",
    "    from kan_utils.fasterkan import FasterKAN\n",
    "    from kan_utils.training import initialize_kan_model_from_config, train_and_validate_model, validate_model\n",
    "    from kan_utils.general_utils import load_json, count_parameters, get_model_macs_params\n",
    "    from kan_utils.plotter import loss_plotter, accuracy_plotter\n",
    "except ImportError as e:\n",
    "    print(f\"Import error: {e}\")\n",
    "\n",
    "# Set device based on settings\n",
    "device = torch.device('cuda' if torch.cuda.is_available() and USE_CUDA else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f99cee",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "We'll load the MNIST dataset using torchvision and prepare it for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7d4e4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 48000\n",
      "Validation dataset size: 12000\n",
      "Test dataset size: 10000\n",
      "Number of batches in training loader: 375\n",
      "Number of batches in validation loader: 94\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "config_path = 'config.json'\n",
    "config = load_json(config_path)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Data transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # MNIST mean and std\n",
    "])\n",
    "\n",
    "# Load MNIST dataset using the specified root directory\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=DATASET_ROOT, \n",
    "    train=True, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=DATASET_ROOT, \n",
    "    train=False, \n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Split training data into train and validation sets (80% train, 20% validation)\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=config['batch_size'], \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=config['batch_size'], \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=config['batch_size'], \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "print(f\"Number of batches in training loader: {len(train_loader)}\")\n",
    "print(f\"Number of batches in validation loader: {len(val_loader)}\")\n",
    "print(f\"Number of classes: {config['output_classes']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e813c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x300 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize some sample images\n",
    "def imshow(img):\n",
    "    \"\"\"Display a PyTorch tensor as an image.\"\"\"\n",
    "    img = img / 2 + 0.5  # Unnormalize\n",
    "    npimg = img.numpy()\n",
    "    \n",
    "    # Handle different image dimensions:\n",
    "    # If the image is a single-channel grayscale image (which MNIST is)\n",
    "    if npimg.ndim == 2:\n",
    "        plt.imshow(npimg, cmap='gray')\n",
    "    elif npimg.ndim == 3 and npimg.shape[0] == 1:  # Batched single-channel\n",
    "        plt.imshow(npimg.squeeze(), cmap='gray')\n",
    "    else:  # RGB or other multi-channel image\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    \n",
    "    plt.axis('off')\n",
    "\n",
    "# Get random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Show images\n",
    "plt.figure(figsize=(12, 3))\n",
    "for i in range(8):\n",
    "    plt.subplot(1, 8, i + 1)\n",
    "    imshow(images[i].squeeze())\n",
    "    plt.title(str(labels[i].item()))\n",
    "plt.suptitle(\"Sample MNIST Digits\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9350c6ae",
   "metadata": {},
   "source": [
    "## 2. Model Setup\n",
    "\n",
    "Now let's initialize our FasterKAN model using the configuration from `config.json`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1711515c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "               RSF-1               [-1, 196, 4]               0\n",
      "           Dropout-2                  [-1, 784]               0\n",
      "            Linear-3                  [-1, 128]         100,352\n",
      "    FasterKANLayer-4                  [-1, 128]               0\n",
      "               RSF-5               [-1, 128, 4]               0\n",
      "           Dropout-6                  [-1, 512]               0\n",
      "            Linear-7                   [-1, 10]           5,120\n",
      "    FasterKANLayer-8                   [-1, 10]               0\n",
      "================================================================\n",
      "Total params: 105,472\n",
      "Trainable params: 105,472\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.02\n",
      "Params size (MB): 0.40\n",
      "Estimated Total Size (MB): 0.42\n",
      "----------------------------------------------------------------\n",
      "Model architecture: FasterKAN(\n",
      "  (layers): ModuleList(\n",
      "    (0): FasterKANLayer(\n",
      "      (rbf): RSF()\n",
      "      (linear): Linear(in_features=784, out_features=128, bias=False)\n",
      "      (drop): Dropout(p=0.68359375, inplace=False)\n",
      "    )\n",
      "    (1): FasterKANLayer(\n",
      "      (rbf): RSF()\n",
      "      (linear): Linear(in_features=512, out_features=10, bias=False)\n",
      "      (drop): Dropout(p=0.68359375, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Total parameters: 105,482\n",
      "Trainable parameters: 105,482\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n",
      "MACs: 105,472.0\n",
      "Checkpoint directory: Training Checkpoints\\CrossEntropyLoss_AdamW_ReduceLROnPlateau_0.003_[4]_c4165b3a\n"
     ]
    }
   ],
   "source": [
    "# Initialize model from config\n",
    "model, checkpoint_dir = initialize_kan_model_from_config(config_path, device=device)\n",
    "\n",
    "# Alternative direct initialization (if you want to customize the model)\n",
    "'''\n",
    "model = FasterKAN(\n",
    "    layers_hidden=config['dim_list'],\n",
    "    num_grids=config['grid_size_per_layer'],\n",
    "    grid_min=config['grid_min'],\n",
    "    grid_max=config['grid_max'],\n",
    "    inv_denominator=config['inv_denominator']\n",
    ").to(device)\n",
    "'''\n",
    "\n",
    "# Print model summary\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "print(f\"Model architecture: {model}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "# Calculate MACs (if possible)\n",
    "macs, _, _ = get_model_macs_params(model, config)\n",
    "if macs:\n",
    "    print(f\"MACs: {macs:,}\")\n",
    "\n",
    "# Ensure checkpoint directory exists\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "print(f\"Checkpoint directory: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945e4e18",
   "metadata": {},
   "source": [
    "## 3. Training\n",
    "\n",
    "Now we'll train the model using the `train_and_validate_model` function from the `kan_utils` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1290d1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mrlnp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5f0a98d3f64d1f84ff50f8ff6e059f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch [1/15]:   0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (128x3136 and 784x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_validate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcriterion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimizer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscheduler\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscheduler_patience\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfactor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscheduler_factor\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpatience\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscheduler_patience\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mverbose\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Save model checkpoint every 5 epochs,\u001b[39;49;00m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_one_hot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Whether to use one-hot encoding for targets, not needed for CrossEntropyLoss\u001b[39;49;00m\n\u001b[0;32m     22\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mrlnp\\OneDrive - National and Kapodistrian University of Athens\\Υπολογιστής\\KANs\\Radial-Basis-Function-Based-Kolmogorov-Arnold-Networks\\kan_utils\\training.py:428\u001b[0m, in \u001b[0;36mtrain_and_validate_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, scheduler, device, checkpoint_dir, epochs, start_epoch, patience, learning_rate, early_stopping, scheduler_params, save_every, use_one_hot)\u001b[0m\n\u001b[0;32m    424\u001b[0m data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    426\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 428\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_one_hot:\n\u001b[0;32m    430\u001b[0m     targets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mone_hot(target, output\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mto(output\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\mrlnp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mrlnp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mrlnp\\OneDrive - National and Kapodistrian University of Athens\\Υπολογιστής\\KANs\\Radial-Basis-Function-Based-Kolmogorov-Arnold-Networks\\kan_utils\\fasterkan.py:309\u001b[0m, in \u001b[0;36mFasterKAN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;124;03m    x (torch.Tensor): Input tensor [batch_size, input_dim]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m    torch.Tensor: Output tensor [batch_size, output_dim]\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 309\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\mrlnp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mrlnp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mrlnp\\OneDrive - National and Kapodistrian University of Athens\\Υπολογιστής\\KANs\\Radial-Basis-Function-Based-Kolmogorov-Arnold-Networks\\kan_utils\\fasterkan.py:216\u001b[0m, in \u001b[0;36mFasterKANLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    214\u001b[0m spline_basis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrbf(x)\u001b[38;5;241m.\u001b[39mview(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    215\u001b[0m spline_basis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(spline_basis)\n\u001b[1;32m--> 216\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspline_basis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Users\\mrlnp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\mrlnp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mrlnp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (128x3136 and 784x128)"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_history = train_and_validate_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=config['criterion'],\n",
    "    optimizer=config['optimizer'],\n",
    "    scheduler=config.get('scheduler'),\n",
    "    learning_rate=config.get('learning_rate'),\n",
    "    device=device,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    "    epochs=15,\n",
    "    patience=config.get('scheduler_patience', 5),\n",
    "    early_stopping=True,\n",
    "    scheduler_params={\n",
    "        'factor': config.get('scheduler_factor', 0.5),\n",
    "        'patience': config.get('scheduler_patience', 5),\n",
    "        'verbose': True\n",
    "    },\n",
    "    save_every=5,  # Save model checkpoint every 5 epochs,\n",
    "    use_one_hot=False # Whether to use one-hot encoding for targets, not needed for CrossEntropyLoss\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9142f99d",
   "metadata": {},
   "source": [
    "## 4. Visualization\n",
    "\n",
    "Let's visualize the training results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39a964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_loss_curves(train_history)\n",
    "plt.title('Loss Curves')\n",
    "\n",
    "# Plot training and validation accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_accuracy_curves(train_history)\n",
    "plt.title('Accuracy Curves')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bef8b77",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation\n",
    "\n",
    "Let's evaluate our model on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2cb1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model checkpoint\n",
    "best_checkpoint = os.path.join(checkpoint_dir, 'best', 'model_checkpoint.pth')\n",
    "if os.path.exists(best_checkpoint):\n",
    "    # Evaluate the model on test data\n",
    "    test_loss, test_accuracy, test_results = validate_model(\n",
    "        model=model,\n",
    "        val_loader=test_loader,\n",
    "        criterion=config['criterion'],\n",
    "        checkpoint_path=best_checkpoint,\n",
    "        device=device,\n",
    "        metrics_flag=True,\n",
    "        return_predictions=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    y_true = test_results['y_true']\n",
    "    y_pred = test_results['y_pred']\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plot_confusion_matrix(cm, range(10))\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print classification report\n",
    "    target_names = [str(i) for i in range(10)]\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "else:\n",
    "    print(\"Best model checkpoint not found. Please train the model first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86444643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some sample predictions\n",
    "def show_predictions(model, data_loader, device, num_samples=10):\n",
    "    \"\"\"Show model predictions on sample images.\"\"\"\n",
    "    model.eval()\n",
    "    dataiter = iter(data_loader)\n",
    "    images, labels = next(dataiter)\n",
    "    \n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        outputs = model(images.view(images.size(0), -1))\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "    \n",
    "    # Plot images with predictions\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    for i in range(num_samples):\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        imshow(images[i].cpu().squeeze())\n",
    "        color = 'green' if predicted[i] == labels[i] else 'red'\n",
    "        plt.title(f'Pred: {predicted[i].item()}\\nTrue: {labels[i].item()}', color=color)\n",
    "    \n",
    "    plt.suptitle(\"Model Predictions on Test Images\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show some predictions\n",
    "try:\n",
    "    show_predictions(model, test_loader, device)\n",
    "except Exception as e:\n",
    "    print(f\"Error displaying predictions: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d32225",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "In this notebook, we have:\n",
    "\n",
    "1. Loaded and prepared the MNIST dataset\n",
    "2. Initialized a FasterKAN model with a [784, 128, 10] architecture\n",
    "3. Trained the model for 10 epochs with validation\n",
    "4. Visualized the training metrics\n",
    "5. Evaluated the model on the test dataset\n",
    "6. Displayed sample predictions\n",
    "\n",
    "The RBF-KAN model demonstrates effective performance on the MNIST digit classification task. The architecture's unique use of radial basis functions provides an alternative approach to traditional neural networks.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Experiment with different hyperparameters (grid size, dimensions, etc.)\n",
    "- Try model quantization (see `mnist_kan_quant.ipynb`)\n",
    "- Compare with other model architectures"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2289c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader, random_split, WeightedRandomSampler\n",
    "\n",
    "# Local\n",
    "from checkpoint_config import load_checkpoint, checkpoint_dir_name, save_checkpoint\n",
    "from training_and_val import initialize_model, validate_model\n",
    "from fasterkan import RSF\n",
    "\n",
    "device = torch.device(\"cpu\") \n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# ---------------------- Check RBF's Values ----------------------\n",
    "\n",
    "def check_rbf_parameters(model):\n",
    "    print(\"\\nRBF Parameters in Model State Dict:\")\n",
    "    print(\"=\" * 40)\n",
    "    state_dict = model.state_dict()\n",
    "    for key, value in state_dict.items():\n",
    "        if \"rbf.grid\" in key or \"rbf.inv_denominator\" in key:\n",
    "            print(f\"{key}: {value}\")\n",
    "    print(\"=\" * 40)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d46248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_pth = r'Training Checkpoints Poco\\Pretrained\\45482\\BCELoss\\Adam\\ReduceOnPlateau\\3.0e-05\\[12288,1024,7]\\[4]\\-2.0e+00\\2.5e-01\\1.5e+00\\epoch_best\\model_checkpoint.pth'\n",
    "\n",
    "dataset_path = 'Dataset'\n",
    "csv_path = os.path.join(dataset_path, \"HAM10000_metadata.csv\")\n",
    "image_dir = os.path.join(dataset_path, \"HAM10000_images\")\n",
    "image_test_dir = image_dir\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "unique_diagnoses = df['dx'].unique()\n",
    "num_classes = len(unique_diagnoses)\n",
    "\n",
    "nv_df = df[df['dx'] == 'nv'].reset_index()\n",
    "df = df[~(df['dx'] == 'nv')].reset_index()\n",
    "\n",
    "# Root Directory to save the training checkpoints\n",
    "root_dir = r\"Dataset/FX-Quantizer\"\n",
    "os.makedirs(root_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9ecf22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Input Dimensions for the model:\n",
    "x_dim, y_dim = 64,64\n",
    "channel_size = 3\n",
    "batch_size = 64\n",
    "seed = 45482\n",
    "\n",
    "torch.manual_seed(seed=seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Hyperparameter sweep configs\n",
    "grid_sizes = [[4]]\n",
    "learning_rates = [3.0e-05,]\n",
    "hidden_layer_configs = [[1024,]]\n",
    "epochs = 10\n",
    "criterion_type = 'BCELoss'\n",
    "optim_type = 'Adam'\n",
    "sched_type = 'ReduceOnPlateau'\n",
    "\n",
    "grid_min_list = [-2,]\n",
    "grid_max_list = [0.25,]\n",
    "inv_denominator_list = [1.5,2.0,2.5]\n",
    "\n",
    "probability = 0.25\n",
    "pretrained = True\n",
    "\n",
    "pretrained_top_dir = root_dir.replace('FX-Quantizer', 'Pretrained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7447a0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkinCancerDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, csv_path, output_classes, transform=None):\n",
    "\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        if isinstance(csv_path, str):\n",
    "            df = pd.read_csv(csv_path)\n",
    "        else:\n",
    "            df = csv_path\n",
    "        df = df.reindex()\n",
    "        self.image_files = [os.path.splitext(f)[0] for f in os.listdir(root) if f.endswith('.jpg')]\n",
    "\n",
    "        assert np.sum(df['image_id'].isin(self.image_files)) == len(df)\n",
    "        self.image_files = df['image_id'].values.tolist()\n",
    "        # Map class names to integer indices\n",
    "        classes = df['dx'].unique()\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "        \n",
    "        self.labels = [self.class_to_idx[cls] for cls in df['dx'].values]\n",
    "        self.output_classes = output_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.root, f'{self.image_files[index]}.jpg')\n",
    "        image = np.asarray(Image.open(img_path).convert(\"RGB\"))\n",
    "        label = self.labels[index]\n",
    "        if self.transform:\n",
    "            image = self.transform(image = image)\n",
    "        if isinstance(image, dict):\n",
    "            image = image['image']\n",
    "        return image.to(torch.float32), label\n",
    "\n",
    "\n",
    "# Define transforms\n",
    "basic_transform = A.Compose([\n",
    "    A.Resize(y_dim,x_dim),\n",
    "    *([A.ToGray(channel_size,p=1),] if  channel_size == 1 else []),\n",
    "    A.Normalize(),\n",
    "    A.ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Define the augmentation pipeline\n",
    "augmented_transform = A.Compose([\n",
    "    A.Resize(y_dim,x_dim),\n",
    "    A.RandomResizedCrop(size=(x_dim,y_dim), scale=(0.08, 1.0), p=probability),\n",
    "    A.HorizontalFlip(p=probability),\n",
    "    A.VerticalFlip(p=probability),\n",
    "    A.RGBShift(p=probability),\n",
    "    A.RandomSunFlare(p=probability),\n",
    "    A.RandomBrightnessContrast(p=probability),\n",
    "    A.HueSaturationValue(p=probability),\n",
    "    A.ColorJitter(p=probability),\n",
    "    A.RandomRotate90(p=probability),\n",
    "    A.Perspective(p=probability),\n",
    "    A.MotionBlur(p=probability),\n",
    "    A.ChannelShuffle(p=probability),\n",
    "    A.ChannelDropout(p=probability),\n",
    "    *([A.ToGray(channel_size,p=1),] if  channel_size == 1 else []),\n",
    "    A.Normalize(),\n",
    "    A.ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff1e2675",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [0.75, 0.09, 0.16]\n",
    "full_dataset = SkinCancerDataset(root=image_dir, csv_path=df, output_classes=num_classes,transform=None)\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, splits)\n",
    "\n",
    "if pretrained:\n",
    "    df = pd.concat([df, nv_df]).reset_index()\n",
    "    full_dataset = SkinCancerDataset(root=image_dir, csv_path=df, output_classes=num_classes,transform=None)\n",
    "    \n",
    "    train_dataset.dataset = full_dataset\n",
    "    val_dataset.dataset = full_dataset\n",
    "    test_dataset.dataset = full_dataset\n",
    "    \n",
    "    nv_ind = df[df['dx'] == 'nv'].index.to_list()\n",
    "    tr_nv_ind, val_nv_ind, test_nv_ind = random_split(nv_ind, splits)\n",
    "\n",
    "    train_dataset.indices += tr_nv_ind.indices\n",
    "    val_dataset.indices += val_nv_ind.indices\n",
    "    test_dataset.indices += test_nv_ind.indices\n",
    "    \n",
    "\n",
    "# Define the transforms for all splits\n",
    "train_dataset.dataset.transform = augmented_transform\n",
    "val_dataset.dataset.transform = basic_transform\n",
    "test_dataset.dataset.transform = basic_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a02a63ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded from Dataset/Pretrained/45482/BCELoss/Adam/ReduceOnPlateau/3.0e-05/[12288,1024,7]/[4]/-2.0e+00/2.5e-01/1.5e+00/epoch_best/model_checkpoint.pth. Next epoch is 498.\n",
      "\n",
      "RBF Parameters in Model State Dict:\n",
      "========================================\n",
      "layers.0.rbf.grid: tensor([-1.8899, -1.1920, -0.5019,  0.2129])\n",
      "layers.0.rbf.inv_denominator: 1.586333990097046\n",
      "layers.1.rbf.grid: tensor([-1.8616, -1.3367, -0.5240,  0.1764])\n",
      "layers.1.rbf.inv_denominator: 1.4424347877502441\n",
      "========================================\n",
      "\n",
      "Checkpoint loaded from Dataset/Pretrained/45482/BCELoss/Adam/ReduceOnPlateau/3.0e-05/[12288,1024,7]/[4]/-2.0e+00/2.5e-01/2.0e+00/epoch_best/model_checkpoint.pth. Next epoch is 488.\n",
      "\n",
      "RBF Parameters in Model State Dict:\n",
      "========================================\n",
      "layers.0.rbf.grid: tensor([-1.8921, -1.1864, -0.4808,  0.2277])\n",
      "layers.0.rbf.inv_denominator: 2.073580026626587\n",
      "layers.1.rbf.grid: tensor([-1.8617, -1.2484, -0.5892,  0.1851])\n",
      "layers.1.rbf.inv_denominator: 1.9153474569320679\n",
      "========================================\n",
      "\n",
      "Checkpoint loaded from Dataset/Pretrained/45482/BCELoss/Adam/ReduceOnPlateau/3.0e-05/[12288,1024,7]/[4]/-2.0e+00/2.5e-01/2.5e+00/epoch_best/model_checkpoint.pth. Next epoch is 487.\n",
      "\n",
      "RBF Parameters in Model State Dict:\n",
      "========================================\n",
      "layers.0.rbf.grid: tensor([-1.8891, -1.1840, -0.4770,  0.2285])\n",
      "layers.0.rbf.inv_denominator: 2.581298828125\n",
      "layers.1.rbf.grid: tensor([-1.8545, -1.1601, -0.6081,  0.1981])\n",
      "layers.1.rbf.inv_denominator: 2.4012045860290527\n",
      "========================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- Load Pre-Trained Floating Point Model ----------------------\n",
    "models_fp = []\n",
    "checkpoints_fp = []\n",
    "dimension_list = [x_dim * y_dim * channel_size] + hidden_layer_configs[0] + [num_classes]\n",
    "\n",
    "for inv_denom in inv_denominator_list:\n",
    "    path_to_pth = checkpoint_dir_name(criterion=criterion_type,\n",
    "                                    optimizer=optim_type,\n",
    "                                    scheduler=sched_type,\n",
    "                                    seed=seed,\n",
    "                                    dim_list=dimension_list,\n",
    "                                    grid_size = grid_sizes[0],\n",
    "                                    grid_min=grid_min_list[0],\n",
    "                                    grid_max=grid_max_list[0],\n",
    "                                    inv_denominator=inv_denom,\n",
    "                                    root_dir=pretrained_top_dir,\n",
    "                                    learning_rate=learning_rates[0],\n",
    "                                    )\n",
    "    \n",
    "    model_tmp, _ = initialize_model(\n",
    "        root_dir=None,\n",
    "        dimension=dimension_list,\n",
    "        grid_size=grid_sizes[0][0],\n",
    "        lr=learning_rates[0],\n",
    "        sched=sched_type,\n",
    "        optim=optim_type,\n",
    "        criterion=criterion_type,\n",
    "        grid_min=grid_min_list[0],\n",
    "        grid_max=grid_max_list[0],\n",
    "        inv_denominator=inv_denom,\n",
    "        x_dim=x_dim,\n",
    "        y_dim=y_dim,\n",
    "        channel_size=channel_size,\n",
    "        seed=seed\n",
    "    )\n",
    "    path_to_pth = os.path.join(path_to_pth, 'epoch_best','model_checkpoint.pth')\n",
    "    model_tmp, *_ = load_checkpoint(model_tmp, optimizer_name=optim_type, checkpoint_path=path_to_pth, device='cpu')\n",
    "    models_fp.append(model_tmp)\n",
    "    checkpoint_dir = os.path.dirname(path_to_pth).replace(pretrained_top_dir,root_dir)\n",
    "    checkpoints_fp.append(checkpoint_dir)\n",
    "    check_rbf_parameters(model_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97630d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = full_dataset.class_to_idx.keys()\n",
    "\n",
    "# Calculate class weights using the training set \n",
    "train_indices = train_dataset.indices # Get indices of the training samples\n",
    "train_labels = df.loc[train_indices, 'dx']\n",
    "\n",
    "class_sample_counts = train_labels.value_counts().reindex(classes, fill_value=0).values\n",
    "class_weights = 1.0 / torch.tensor(class_sample_counts, dtype=torch.float)\n",
    "train_labels = train_labels.tolist()\n",
    "\n",
    "dataset_len = int(len(classes) / np.sqrt(np.mean((1 / class_sample_counts) ** 2)))\n",
    "\n",
    "# Map class names to indices\n",
    "label_to_idx = full_dataset.class_to_idx\n",
    "train_label_indices = [label_to_idx[label] for label in train_labels]\n",
    "\n",
    "num_workers = os.cpu_count()\n",
    "# num_workers = 0\n",
    "\n",
    "torch.manual_seed(seed=seed)\n",
    "sample_weights = [class_weights[label_idx].item() for label_idx in train_label_indices]\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=dataset_len, replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bf12447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 89.69%\n",
    "calibration_set_size = 1000\n",
    "calibration_indices = list(range(calibration_set_size))\n",
    "calibration_subset = torch.utils.data.Subset(train_dataset, calibration_indices)\n",
    "calibration_loader = DataLoader(calibration_subset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 89.57%\n",
    "# calibration_set_size = 1000\n",
    "# torch.manual_seed(seed=seed)\n",
    "# calibration_sampler = WeightedRandomSampler(sample_weights, num_samples=calibration_set_size, replacement=False) # Use a WeightedRandomSampler to select calibration indices\n",
    "# calibration_loader = DataLoader(train_dataset, batch_size=64, sampler=calibration_sampler, num_workers=num_workers)\n",
    "\n",
    "# 89.51%\n",
    "# # Create a calibration subset with equal parts per class (stratified sampling)\n",
    "# from collections import defaultdict\n",
    "\n",
    "# calibration_set_size = 1000  # total calibration samples\n",
    "# num_classes = len(classes)\n",
    "# samples_per_class = calibration_set_size // num_classes\n",
    "\n",
    "# # Find indices for each class in the training set\n",
    "# class_to_indices = defaultdict(list)\n",
    "# for idx in train_dataset.indices:\n",
    "#     label = df.loc[idx, 'dx']\n",
    "#     class_to_indices[label].append(idx)\n",
    "\n",
    "# # Sample equal number of indices per class\n",
    "# calibration_indices = []\n",
    "# for cls in classes:\n",
    "#     indices = class_to_indices[cls]\n",
    "#     calibration_indices.extend(indices[:samples_per_class])\n",
    "\n",
    "# # Create the calibration subset and loader\n",
    "# calibration_subset = torch.utils.data.Subset(train_dataset.dataset, calibration_indices)\n",
    "# calibration_loader = DataLoader(calibration_subset, batch_size=batch_size, shuffle=False, num_workers=num_workers) \n",
    "\n",
    "# ---------------------- Quantization of RSF Module ----------------------\n",
    "import copy\n",
    "from torch.ao.quantization import quantize_fx, get_default_qconfig_mapping\n",
    "\n",
    "class RSFQuant(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rsf_module\n",
    "    ):\n",
    "        super(RSFQuant, self).__init__()\n",
    "        self.grid = torch.nn.Parameter(rsf_module.grid.data.clone().detach(), requires_grad=False)\n",
    "        self.inv_denominator = torch.nn.Parameter(torch.tensor(rsf_module.inv_denominator.data.clone().detach(), dtype=torch.float32), requires_grad=False)  # Cache the inverse of the denominator\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Compute the forward pass\n",
    "        diff_mul = (x[..., None] - self.grid) * self.inv_denominator\n",
    "        tanh_diff = self.tanh(diff_mul)\n",
    "        tanh_diff_deriviative = 1. - tanh_diff ** 2  # sech^2(x) = 1 - tanh^2(x)\n",
    "        \n",
    "        return tanh_diff_deriviative\n",
    "\n",
    "def convert_to_quantizable(model):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, RSF):\n",
    "            setattr(model, name, RSFQuant(module)) # Replace RSF with RSFQuant and copy parameters\n",
    "        else:\n",
    "            convert_to_quantizable(module) # Recursively process submodules\n",
    "    return model\n",
    "\n",
    "# ---------------------- Quantization Pipeline ----------------------\n",
    "\n",
    "def quantize_model_and_calibrate(model_fp, calibration_loader, checkpoint_dir, device='cpu'):\n",
    "    \"\"\"\n",
    "    Perform quantization on a given model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model_fp: The original full-precision model.\n",
    "    - calibration_loader: DataLoader for calibration.\n",
    "    - device: Device to perform quantization on.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy and convert to a quantizable model\n",
    "    model_to_quantize = copy.deepcopy(model_fp.cpu())\n",
    "    model_to_quantize = convert_to_quantizable(model_to_quantize)  # Switch RSF with RSFQuant\n",
    "    \n",
    "    model_fp.eval()\n",
    "    model_to_quantize.eval()\n",
    "    \n",
    "    # Orepare, Calibrate, Convert\n",
    "    model_prepared = quantize_fx.prepare_fx(model_to_quantize, get_default_qconfig_mapping(\"fbgemm\"), calibration_loader)  \n",
    "    model_prepared.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in calibration_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            _ = model_prepared(inputs)\n",
    "    \n",
    "    model_quantized_static = quantize_fx.convert_fx(model_prepared.cpu())\n",
    "    save_checkpoint(checkpoint_dir, model_quantized_static, optimizer=None, epoch='FX-Quantize', loss=None, best_val_loss=None, device=device)\n",
    "    \n",
    "    return model_quantized_static.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f39b94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_size(mdl):\n",
    "    torch.save(mdl.state_dict(), \"tmp.pt\")\n",
    "    size = os.path.getsize(\"tmp.pt\") / 1e6\n",
    "    os.remove(\"tmp.pt\")\n",
    "    return f\"{size:.2f} MB\"\n",
    "\n",
    "# Quantize all models in models_fp\n",
    "model_quantized_static_list = [\n",
    "    quantize_model_and_calibrate(model_fp=mdl, checkpoint_dir=checkpoint_dir, calibration_loader=calibration_loader, device=device)\n",
    "    for mdl,checkpoint_dir in zip(models_fp,checkpoints_fp)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b86b312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26686/3724971397.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.inv_denominator = torch.nn.Parameter(torch.tensor(rsf_module.inv_denominator.data.clone().detach(), dtype=torch.float32), requires_grad=False)  # Cache the inverse of the denominator\n",
      "/home/gvenit/Projects/kan-fpga/test/.venv/lib/python3.12/site-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Sizes\n",
      "========================================\n",
      "Model 0 size (floating point): 201.44 MB\n",
      "Model 0 size (quantized): 50.39 MB\n",
      "Model 1 size (floating point): 201.44 MB\n",
      "Model 1 size (quantized): 50.39 MB\n",
      "Model 2 size (floating point): 201.44 MB\n",
      "Model 2 size (quantized): 50.39 MB\n",
      "========================================\n",
      "\n",
      "Model Evaluation Results\n",
      "========================================\n",
      "Checkpoint loaded from Dataset/Pretrained/45482/BCELoss/Adam/ReduceOnPlateau/3.0e-05/[12288,1024,7]/[4]/-2.0e+00/2.5e-01/1.5e+00/epoch_best/model_checkpoint.pth. Next epoch is 498.\n",
      "Model loaded from checkpoint: Dataset/Pretrained/45482/BCELoss/Adam/ReduceOnPlateau/3.0e-05/[12288,1024,7]/[4]/-2.0e+00/2.5e-01/1.5e+00/epoch_best/model_checkpoint.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c031ede5f54b438bb1b53a6bb7615281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/26 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8982\n",
      "Recall: 0.8982\n",
      "Accuracy: 89.82%\n",
      "Confusion Matrix:\n",
      "[[289   2  23   1  12  11  11]\n",
      " [  4  28   2   0   4   0   0]\n",
      " [ 30   2 324   5   4   5   1]\n",
      " [  2   0   0  34   0   0   0]\n",
      " [  5   3   4   0 157   2   1]\n",
      " [  3   2   4   0   7  80   1]\n",
      " [  5   3   2   0   0   2 526]]\n",
      "Checkpoint loaded from Dataset/FX-Quantizer/45482/BCELoss/Adam/ReduceOnPlateau/3.0e-05/[12288,1024,7]/[4]/-2.0e+00/2.5e-01/1.5e+00/epoch_best/model_checkpoint.pth. Next epoch is FX-Quantize.\n",
      "Model loaded from checkpoint: Dataset/FX-Quantizer/45482/BCELoss/Adam/ReduceOnPlateau/3.0e-05/[12288,1024,7]/[4]/-2.0e+00/2.5e-01/1.5e+00/epoch_best/model_checkpoint.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gvenit/Projects/kan-fpga/test/.venv/lib/python3.12/site-packages/torch/_utils.py:413: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  device=storage.device,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00356309448c448ebda350fa1166f7b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/26 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8968\n",
      "Recall: 0.8969\n",
      "Accuracy: 89.69%\n",
      "Confusion Matrix:\n",
      "[[294   0  27   0  10   7  11]\n",
      " [  4  28   3   0   3   0   0]\n",
      " [ 31   1 324   5   3   5   2]\n",
      " [  1   0   1  34   0   0   0]\n",
      " [  9   1   4   0 155   2   1]\n",
      " [  5   2   4   0   7  78   1]\n",
      " [  8   3   2   0   0   2 523]]\n",
      "Model path : Dataset/FX-Quantizer/45482/BCELoss/Adam/ReduceOnPlateau/3.0e-05/[12288,1024,7]/[4]/-2.0e+00/2.5e-01/1.5e+00/epoch_best\n",
      "Model 0 Floating Point:\n",
      "  - Validation Loss: 0.0882\n",
      "  - Accuracy: 89.82%\n",
      "Model 0 Quantized:\n",
      "  - Validation Loss: 0.0907\n",
      "  - Accuracy: 89.69%\n",
      "----------------------------------------\n",
      "Checkpoint loaded from Dataset/Pretrained/45482/BCELoss/Adam/ReduceOnPlateau/3.0e-05/[12288,1024,7]/[4]/-2.0e+00/2.5e-01/2.0e+00/epoch_best/model_checkpoint.pth. Next epoch is 488.\n",
      "Model loaded from checkpoint: Dataset/Pretrained/45482/BCELoss/Adam/ReduceOnPlateau/3.0e-05/[12288,1024,7]/[4]/-2.0e+00/2.5e-01/2.0e+00/epoch_best/model_checkpoint.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46b528a3a008480c9210cab7693d1f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/26 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9064\n",
      "Recall: 0.9063\n",
      "Accuracy: 90.63%\n",
      "Confusion Matrix:\n",
      "[[301   0  24   0   7   7  10]\n",
      " [  6  28   1   0   3   0   0]\n",
      " [ 27   2 328   3   3   5   3]\n",
      " [  2   0   0  33   1   0   0]\n",
      " [  4   3   4   0 156   4   1]\n",
      " [  1   2   3   0   7  83   1]\n",
      " [  7   3   2   0   1   3 522]]\n",
      "Checkpoint loaded from Dataset/FX-Quantizer/45482/BCELoss/Adam/ReduceOnPlateau/3.0e-05/[12288,1024,7]/[4]/-2.0e+00/2.5e-01/2.0e+00/epoch_best/model_checkpoint.pth. Next epoch is FX-Quantize.\n",
      "Model loaded from checkpoint: Dataset/FX-Quantizer/45482/BCELoss/Adam/ReduceOnPlateau/3.0e-05/[12288,1024,7]/[4]/-2.0e+00/2.5e-01/2.0e+00/epoch_best/model_checkpoint.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c44af5781747f8aba22ab37b5136b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/26 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9053\n",
      "Recall: 0.9051\n",
      "Accuracy: 90.51%\n",
      "Confusion Matrix:\n",
      "[[303   0  26   0   7   3  10]\n",
      " [  6  28   1   0   3   0   0]\n",
      " [ 27   0 336   3   1   3   1]\n",
      " [  3   0   0  32   0   0   1]\n",
      " [  6   1   4   0 156   4   1]\n",
      " [  1   2   5   0   6  82   1]\n",
      " [ 13   3   3   0   3   4 512]]\n",
      "Model path : Dataset/FX-Quantizer/45482/BCELoss/Adam/ReduceOnPlateau/3.0e-05/[12288,1024,7]/[4]/-2.0e+00/2.5e-01/2.0e+00/epoch_best\n",
      "Model 1 Floating Point:\n",
      "  - Validation Loss: 0.0820\n",
      "  - Accuracy: 90.63%\n",
      "Model 1 Quantized:\n",
      "  - Validation Loss: 0.0851\n",
      "  - Accuracy: 90.51%\n",
      "----------------------------------------\n",
      "Checkpoint loaded from Dataset/Pretrained/45482/BCELoss/Adam/ReduceOnPlateau/3.0e-05/[12288,1024,7]/[4]/-2.0e+00/2.5e-01/2.5e+00/epoch_best/model_checkpoint.pth. Next epoch is 487.\n",
      "Model loaded from checkpoint: Dataset/Pretrained/45482/BCELoss/Adam/ReduceOnPlateau/3.0e-05/[12288,1024,7]/[4]/-2.0e+00/2.5e-01/2.5e+00/epoch_best/model_checkpoint.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd1c2bcad3964d1da68e7cbe7f8e9f09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/26 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9123\n",
      "Recall: 0.9126\n",
      "Accuracy: 91.26%\n",
      "Confusion Matrix:\n",
      "[[303   0  23   0   8   1  14]\n",
      " [  6  28   1   0   3   0   0]\n",
      " [ 23   0 336   3   3   3   3]\n",
      " [  5   0   0  30   0   0   1]\n",
      " [  2   1   4   0 160   5   0]\n",
      " [  1   2   4   0   7  82   1]\n",
      " [  6   3   1   0   3   3 522]]\n",
      "Checkpoint loaded from Dataset/FX-Quantizer/45482/BCELoss/Adam/ReduceOnPlateau/3.0e-05/[12288,1024,7]/[4]/-2.0e+00/2.5e-01/2.5e+00/epoch_best/model_checkpoint.pth. Next epoch is FX-Quantize.\n",
      "Model loaded from checkpoint: Dataset/FX-Quantizer/45482/BCELoss/Adam/ReduceOnPlateau/3.0e-05/[12288,1024,7]/[4]/-2.0e+00/2.5e-01/2.5e+00/epoch_best/model_checkpoint.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c69c275f2c1047c9b55d1c5180dc0c09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/26 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9064\n",
      "Recall: 0.9063\n",
      "Accuracy: 90.63%\n",
      "Confusion Matrix:\n",
      "[[310   0  24   0   8   1   6]\n",
      " [  6  28   1   0   3   0   0]\n",
      " [ 22   0 344   0   2   2   1]\n",
      " [  2   0   3  26   2   0   3]\n",
      " [  2   1   5   0 162   2   0]\n",
      " [  3   2   5   0   6  80   1]\n",
      " [ 17   1   8   0   7   4 501]]\n",
      "Model path : Dataset/FX-Quantizer/45482/BCELoss/Adam/ReduceOnPlateau/3.0e-05/[12288,1024,7]/[4]/-2.0e+00/2.5e-01/2.5e+00/epoch_best\n",
      "Model 2 Floating Point:\n",
      "  - Validation Loss: 0.0727\n",
      "  - Accuracy: 91.26%\n",
      "Model 2 Quantized:\n",
      "  - Validation Loss: 0.0806\n",
      "  - Accuracy: 90.63%\n",
      "----------------------------------------\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model sizes\n",
    "print(\"\\nModel Sizes\")\n",
    "print(\"=\" * 40)\n",
    "for i, (fp, q) in enumerate(zip(models_fp, model_quantized_static_list)):\n",
    "    print(f\"Model {i} size (floating point): {model_size(fp)}\")\n",
    "    print(f\"Model {i} size (quantized): {model_size(q)}\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Evaluate accuracy for all models\n",
    "print(\"\\nModel Evaluation Results\")\n",
    "print(\"=\" * 40)\n",
    "for i, (fp, q, cdir) in enumerate(zip(models_fp, model_quantized_static_list,checkpoints_fp)):\n",
    "    pth = os.path.join(cdir, 'model_checkpoint.pth')\n",
    "    val_loss_fp, accuracy_fp = validate_model(model=fp, val_loader=test_loader, criterion=criterion_type, checkpoint_path = pth.replace(root_dir,pretrained_top_dir), device=device, metrics_flag=True)\n",
    "    val_loss_q, accuracy_q = validate_model(model=q, val_loader=test_loader, criterion=criterion_type, checkpoint_path = pth, device='cpu', metrics_flag=True)\n",
    "    print('Model path :', cdir)\n",
    "    print(f\"Model {i} Floating Point:\\n  - Validation Loss: {val_loss_fp:.4f}\\n  - Accuracy: {accuracy_fp:.2f}%\")\n",
    "    print(f\"Model {i} Quantized:\\n  - Validation Loss: {val_loss_q:.4f}\\n  - Accuracy: {accuracy_q:.2f}%\")\n",
    "    print(\"-\" * 40)\n",
    "print(\"=\" * 40)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

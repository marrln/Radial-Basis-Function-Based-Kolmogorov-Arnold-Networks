{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2289c9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from torch.utils.data import DataLoader, random_split, WeightedRandomSampler\n",
    "\n",
    "# Local (updated imports)\n",
    "from checkpoint import load_model_checkpoint, make_checkpoint_dir_from_hyperparams, save_model_checkpoint, build_hyperparams_typedict\n",
    "from training import initialize_kan_model, validate_model, train_and_validate_model, update_logs\n",
    "from fasterkan import RSF\n",
    "from mapper import get_optimizer, get_scheduler, get_criterion\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------------------- Check RBF's Values ----------------------\n",
    "\n",
    "def check_rbf_parameters(model):\n",
    "    print(\"\\nRBF Parameters in Model State Dict:\")\n",
    "    print(\"=\" * 40)\n",
    "    state_dict = model.state_dict()\n",
    "    for key, value in state_dict.items():\n",
    "        if \"rbf.grid\" in key or \"rbf.inv_denominator\" in key:\n",
    "            print(f\"{key}: {value}\")\n",
    "    print(\"=\" * 40)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d46248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_pth = r'Training Checkpoints Poco\\Pretrained\\45482\\BCELoss\\Adam\\ReduceOnPlateau\\3.0e-05\\[12288,1024,7]\\[4]\\-2.0e+00\\2.5e-01\\1.5e+00\\epoch_best\\model_checkpoint.pth'\n",
    "\n",
    "dataset_path = r'C:\\Users\\mrlnp\\OneDrive - National and Kapodistrian University of Athens\\Υπολογιστής\\KANs\\configs\\SKINCANCER\\SkinCancerDataset'\n",
    "csv_path = os.path.join(dataset_path, \"HAM10000_metadata.csv\")\n",
    "image_dir = os.path.join(dataset_path, \"HAM10000_images\")\n",
    "image_test_dir = image_dir\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "unique_diagnoses = df['dx'].unique()\n",
    "num_classes = len(unique_diagnoses)\n",
    "\n",
    "nv_df = df[df['dx'] == 'nv'].reset_index()\n",
    "df = df[~(df['dx'] == 'nv')].reset_index()\n",
    "\n",
    "# Root Directory to save the training checkpoints \n",
    "root_dir = r\"checkpoints\"\n",
    "os.makedirs(root_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9ecf22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_dim, y_dim = 64,64\n",
    "channel_size = 3\n",
    "batch_size = 64\n",
    "seed = 45482\n",
    "\n",
    "torch.manual_seed(seed=seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Hyperparameter sweep configs\n",
    "grid_sizes = [[4]]\n",
    "learning_rates = [3.0e-05,]\n",
    "hidden_layer_configs = [[1024,]]\n",
    "epochs = 10\n",
    "criterion_type = 'BCELoss'\n",
    "optim_type = 'Adam'\n",
    "sched_type = 'ReduceOnPlateau'\n",
    "\n",
    "grid_min_list = [-2,]\n",
    "grid_max_list = [0.25,]\n",
    "inv_denominator_list = [1.5,2.0,2.5]\n",
    "\n",
    "probability = 0.25\n",
    "pretrained = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7447a0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkinCancerDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, csv_path, output_classes, transform=None):\n",
    "\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        if isinstance(csv_path, str):\n",
    "            df = pd.read_csv(csv_path)\n",
    "        else:\n",
    "            df = csv_path\n",
    "        df = df.reindex()\n",
    "        self.image_files = [os.path.splitext(f)[0] for f in os.listdir(root) if f.endswith('.jpg')]\n",
    "\n",
    "        assert np.sum(df['image_id'].isin(self.image_files)) == len(df)\n",
    "        self.image_files = df['image_id'].values.tolist()\n",
    "        # Map class names to integer indices\n",
    "        classes = df['dx'].unique()\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(classes)}\n",
    "        \n",
    "        self.labels = [self.class_to_idx[cls] for cls in df['dx'].values]\n",
    "        self.output_classes = output_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = os.path.join(self.root, f'{self.image_files[index]}.jpg')\n",
    "        image = np.asarray(Image.open(img_path).convert(\"RGB\"))\n",
    "        label = self.labels[index]\n",
    "        if self.transform:\n",
    "            image = self.transform(image = image)\n",
    "        if isinstance(image, dict):\n",
    "            image = image['image']\n",
    "        return image.to(torch.float32), label\n",
    "\n",
    "\n",
    "# Define transforms\n",
    "basic_transform = A.Compose([\n",
    "    A.Resize(y_dim,x_dim),\n",
    "    *([A.ToGray(channel_size,p=1),] if  channel_size == 1 else []),\n",
    "    A.Normalize(),\n",
    "    A.ToTensorV2(),\n",
    "])\n",
    "\n",
    "# Define the augmentation pipeline\n",
    "augmented_transform = A.Compose([\n",
    "    A.Resize(y_dim,x_dim),\n",
    "    A.RandomResizedCrop(size=(x_dim,y_dim), scale=(0.08, 1.0), p=probability),\n",
    "    A.HorizontalFlip(p=probability),\n",
    "    A.VerticalFlip(p=probability),\n",
    "    A.RGBShift(p=probability),\n",
    "    A.RandomSunFlare(p=probability),\n",
    "    A.RandomBrightnessContrast(p=probability),\n",
    "    A.HueSaturationValue(p=probability),\n",
    "    A.ColorJitter(p=probability),\n",
    "    A.RandomRotate90(p=probability),\n",
    "    A.Perspective(p=probability),\n",
    "    A.MotionBlur(p=probability),\n",
    "    A.ChannelShuffle(p=probability),\n",
    "    A.ChannelDropout(p=probability),\n",
    "    *([A.ToGray(channel_size,p=1),] if  channel_size == 1 else []),\n",
    "    A.Normalize(),\n",
    "    A.ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff1e2675",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [0.75, 0.09, 0.16]\n",
    "full_dataset = SkinCancerDataset(root=image_dir, csv_path=df, output_classes=num_classes,transform=None)\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, splits)\n",
    "\n",
    "if pretrained:\n",
    "    df = pd.concat([df, nv_df]).reset_index()\n",
    "    full_dataset = SkinCancerDataset(root=image_dir, csv_path=df, output_classes=num_classes,transform=None)\n",
    "    \n",
    "    train_dataset.dataset = full_dataset\n",
    "    val_dataset.dataset = full_dataset\n",
    "    test_dataset.dataset = full_dataset\n",
    "    \n",
    "    nv_ind = df[df['dx'] == 'nv'].index.to_list()\n",
    "    tr_nv_ind, val_nv_ind, test_nv_ind = random_split(nv_ind, splits)\n",
    "\n",
    "    train_dataset.indices += tr_nv_ind.indices\n",
    "    val_dataset.indices += val_nv_ind.indices\n",
    "    test_dataset.indices += test_nv_ind.indices\n",
    "    \n",
    "\n",
    "# Define the transforms for all splits\n",
    "train_dataset.dataset.transform = augmented_transform\n",
    "val_dataset.dataset.transform = basic_transform\n",
    "test_dataset.dataset.transform = basic_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a02a63ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.dropout.Dropout'>.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Checkpoint file not found at path: checkpoints\\45482\\BCELoss\\Adam\\ReduceOnPlateau\\[12288, 1024, 7]\\3e-05\\[4]\\-2.0\\0.25\\1.5\\epoch_best\\model_checkpoint.pth",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m checkpoint_dir \u001b[38;5;241m=\u001b[39m make_checkpoint_dir_from_hyperparams(hyperparams_typedict, hyperparams, root_dir)\n\u001b[0;32m     25\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(checkpoint_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch_best\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_checkpoint.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 27\u001b[0m model_tmp, _, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitialize_kan_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mroot_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhyperparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchannel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchannel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhyperparams_typedict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperparams_typedict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptim_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m     41\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m models_fp\u001b[38;5;241m.\u001b[39mappend(model_tmp)\n\u001b[0;32m     43\u001b[0m check_rbf_parameters(model_tmp)\n",
      "File \u001b[1;32mc:\\Users\\mrlnp\\OneDrive - National and Kapodistrian University of Athens\\Υπολογιστής\\configs for git\\checkpoint.py:230\u001b[0m, in \u001b[0;36mload_model_checkpoint\u001b[1;34m(model, device, checkpoint_path, optimizer_type, optimizer_params)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, optimizer, start_epoch, loss, best_val_loss\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 230\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoint file not found at path: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcheckpoint_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Checkpoint file not found at path: checkpoints\\45482\\BCELoss\\Adam\\ReduceOnPlateau\\[12288, 1024, 7]\\3e-05\\[4]\\-2.0\\0.25\\1.5\\epoch_best\\model_checkpoint.pth"
     ]
    }
   ],
   "source": [
    "# ---------------------- Load Pre-Trained Floating Point Model ----------------------\n",
    "models_fp = []\n",
    "dimension_list = [x_dim * y_dim * channel_size] + hidden_layer_configs[0] + [num_classes]\n",
    "\n",
    "# Build hyperparams dict for new checkpoint logic\n",
    "hyperparams = {\n",
    "    'seed': seed,\n",
    "    'criterion': criterion_type,\n",
    "    'optimizer': optim_type,\n",
    "    'scheduler': sched_type,\n",
    "    'dim_list': dimension_list,\n",
    "    'learning_rate': learning_rates[0],\n",
    "    'grid_size_per_layer': grid_sizes[0],\n",
    "    'grid_min': grid_min_list[0],\n",
    "    'grid_max': grid_max_list[0],\n",
    "    'inv_denominator': inv_denominator_list[0],\n",
    "}\n",
    "hyperparams_typedict = build_hyperparams_typedict(\n",
    "    seed=int, criterion=str, optimizer=str, scheduler=str, dim_list=list, learning_rate=float,\n",
    "    grid_size_per_layer=list, grid_min=float, grid_max=float, inv_denominator=float)\n",
    "\n",
    "for inv_denom in inv_denominator_list:\n",
    "    hyperparams['inv_denominator'] = inv_denom\n",
    "    checkpoint_dir = make_checkpoint_dir_from_hyperparams(hyperparams_typedict, hyperparams, root_dir)\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, 'epoch_best', 'model_checkpoint.pth')\n",
    "    \n",
    "    model_tmp, _, _, _, _ = load_model_checkpoint(\n",
    "        model=initialize_kan_model(\n",
    "            root_dir=root_dir,\n",
    "            hyperparams=hyperparams,\n",
    "            x_dim=x_dim,\n",
    "            y_dim=y_dim,\n",
    "            channel_size=channel_size,\n",
    "            hyperparams_typedict=hyperparams_typedict,\n",
    "            device=device\n",
    "        )[0],\n",
    "        device=device,\n",
    "        checkpoint_path=checkpoint_path,\n",
    "        optimizer_type=optim_type,\n",
    "        optimizer_params=None\n",
    "    )\n",
    "    models_fp.append(model_tmp)\n",
    "    check_rbf_parameters(model_tmp)\n",
    "    print(\"State dict keys:\", list(model_tmp.state_dict().keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97630d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = full_dataset.class_to_idx.keys()\n",
    "\n",
    "# Calculate class weights using the training set \n",
    "train_indices = train_dataset.indices # Get indices of the training samples\n",
    "train_labels = df.loc[train_indices, 'dx']\n",
    "\n",
    "class_sample_counts = train_labels.value_counts().reindex(classes, fill_value=0).values\n",
    "class_weights = 1.0 / torch.tensor(class_sample_counts, dtype=torch.float)\n",
    "train_labels = train_labels.tolist()\n",
    "\n",
    "dataset_len = int(len(classes) / np.sqrt(np.mean((1 / class_sample_counts) ** 2)))\n",
    "\n",
    "# Map class names to indices\n",
    "label_to_idx = full_dataset.class_to_idx\n",
    "train_label_indices = [label_to_idx[label] for label in train_labels]\n",
    "\n",
    "num_workers = os.cpu_count()\n",
    "# num_workers = 0\n",
    "\n",
    "torch.manual_seed(seed=seed)\n",
    "sample_weights = [class_weights[label_idx].item() for label_idx in train_label_indices]\n",
    "sampler = WeightedRandomSampler(sample_weights, num_samples=dataset_len, replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9bf12447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 89.69%\n",
    "calibration_set_size = 1000\n",
    "calibration_indices = list(range(calibration_set_size))\n",
    "calibration_subset = torch.utils.data.Subset(train_dataset, calibration_indices)\n",
    "calibration_loader = DataLoader(calibration_subset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 89.57%\n",
    "# calibration_set_size = 1000\n",
    "# torch.manual_seed(seed=seed)\n",
    "# calibration_sampler = WeightedRandomSampler(sample_weights, num_samples=calibration_set_size, replacement=False) # Use a WeightedRandomSampler to select calibration indices\n",
    "# calibration_loader = DataLoader(train_dataset, batch_size=64, sampler=calibration_sampler, num_workers=num_workers)\n",
    "\n",
    "# 89.51%\n",
    "# # Create a calibration subset with equal parts per class (stratified sampling)\n",
    "# from collections import defaultdict\n",
    "\n",
    "# calibration_set_size = 1000  # total calibration samples\n",
    "# num_classes = len(classes)\n",
    "# samples_per_class = calibration_set_size // num_classes\n",
    "\n",
    "# # Find indices for each class in the training set\n",
    "# class_to_indices = defaultdict(list)\n",
    "# for idx in train_dataset.indices:\n",
    "#     label = df.loc[idx, 'dx']\n",
    "#     class_to_indices[label].append(idx)\n",
    "\n",
    "# # Sample equal number of indices per class\n",
    "# calibration_indices = []\n",
    "# for cls in classes:\n",
    "#     indices = class_to_indices[cls]\n",
    "#     calibration_indices.extend(indices[:samples_per_class])\n",
    "\n",
    "# # Create the calibration subset and loader\n",
    "# calibration_subset = torch.utils.data.Subset(train_dataset.dataset, calibration_indices)\n",
    "# calibration_loader = DataLoader(calibration_subset, batch_size=batch_size, shuffle=False, num_workers=num_workers) \n",
    "\n",
    "# ---------------------- Quantization of RSF Module ----------------------\n",
    "import copy\n",
    "from torch.ao.quantization import quantize_fx, get_default_qconfig_mapping\n",
    "\n",
    "class RSFQuant(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rsf_module\n",
    "    ):\n",
    "        super(RSFQuant, self).__init__()\n",
    "        self.grid = torch.nn.Parameter(rsf_module.grid.detach().clone(), requires_grad=False)\n",
    "        self.inv_denominator = torch.nn.Parameter(torch.tensor(rsf_module.inv_denominator.detach().clone(), dtype=torch.float32, device=device), requires_grad=False)  # Cache the inverse of the denominator\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Compute the forward pass\n",
    "        diff_mul = (x[..., None] - self.grid) * self.inv_denominator\n",
    "        tanh_diff = self.tanh(diff_mul)\n",
    "        tanh_diff_deriviative = 1. - tanh_diff ** 2  # sech^2(x) = 1 - tanh^2(x)\n",
    "        \n",
    "        return tanh_diff_deriviative\n",
    "\n",
    "def convert_to_quantizable(model):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, RSF):\n",
    "            setattr(model, name, RSFQuant(module)) # Replace RSF with RSFQuant and copy parameters\n",
    "        else:\n",
    "            convert_to_quantizable(module) # Recursively process submodules\n",
    "    return model\n",
    "\n",
    "# ---------------------- Quantization Pipeline ----------------------\n",
    "\n",
    "def quantize_model_and_calibrate(model_fp, calibration_loader, device=device):\n",
    "    \"\"\"\n",
    "    Perform quantization on a given model.\n",
    "    \n",
    "    Parameters:\n",
    "    - model_fp: The original full-precision model.\n",
    "    - calibration_loader: DataLoader for calibration.\n",
    "    - device: Device to perform quantization on.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy and convert to a quantizable model\n",
    "    model_to_quantize = copy.deepcopy(model_fp)\n",
    "    model_to_quantize = convert_to_quantizable(model_to_quantize)  # Switch RSF with RSFQuant\n",
    "    \n",
    "    model_fp.eval()\n",
    "    model_to_quantize.eval()\n",
    "    \n",
    "    # Orepare, Calibrate, Convert\n",
    "    model_prepared = quantize_fx.prepare_fx(model_to_quantize, get_default_qconfig_mapping(\"fbgemm\"), calibration_loader)  \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in calibration_loader:\n",
    "            _ = model_prepared(inputs.to(device))\n",
    "    \n",
    "    model_quantized_static = quantize_fx.convert_fx(model_prepared)\n",
    "    torch.save(model_quantized_static.state_dict(), \"model_checkpoint_quantized.pth\")\n",
    "    \n",
    "    return model_quantized_static\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b86b312",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrlnp\\AppData\\Local\\Temp\\ipykernel_16072\\3590222057.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.inv_denominator = torch.nn.Parameter(torch.tensor(rsf_module.inv_denominator.detach().clone(), dtype=torch.float32, device=device), requires_grad=False)  # Cache the inverse of the denominator\n",
      "c:\\Users\\mrlnp\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\ao\\quantization\\observer.py:220: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Sizes\n",
      "========================================\n",
      "Model 0 size (floating point): 201.45 MB\n",
      "Model 0 size (quantized): 50.39 MB\n",
      "Model 1 size (floating point): 201.45 MB\n",
      "Model 1 size (quantized): 50.39 MB\n",
      "Model 2 size (floating point): 201.45 MB\n",
      "Model 2 size (quantized): 50.39 MB\n",
      "========================================\n",
      "\n",
      "Model Evaluation Results\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7a8add6cf9c4725b11ac2574a710f9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/26 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8982\n",
      "Recall: 0.8982\n",
      "Accuracy: 89.82%\n",
      "Confusion Matrix:\n",
      "[[289   2  23   1  12  11  11]\n",
      " [  4  28   2   0   4   0   0]\n",
      " [ 30   2 324   5   4   5   1]\n",
      " [  2   0   0  34   0   0   0]\n",
      " [  5   3   4   0 157   2   1]\n",
      " [  3   2   4   0   7  80   1]\n",
      " [  5   3   2   0   0   2 526]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aff9219473d41e7b5613686e5611f6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/26 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.8968\n",
      "Recall: 0.8969\n",
      "Accuracy: 89.69%\n",
      "Confusion Matrix:\n",
      "[[294   0  27   0  10   7  11]\n",
      " [  4  28   3   0   3   0   0]\n",
      " [ 31   1 324   5   3   5   2]\n",
      " [  1   0   1  34   0   0   0]\n",
      " [  9   1   4   0 155   2   1]\n",
      " [  5   2   4   0   7  78   1]\n",
      " [  8   3   2   0   0   2 523]]\n",
      "Model 0 Floating Point:\n",
      "  - Validation Loss: 0.0882\n",
      "  - Accuracy: 89.82%\n",
      "Model 0 Quantized:\n",
      "  - Validation Loss: 0.0908\n",
      "  - Accuracy: 89.69%\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71005074ad93443c99fa9e217067754e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/26 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9064\n",
      "Recall: 0.9063\n",
      "Accuracy: 90.63%\n",
      "Confusion Matrix:\n",
      "[[301   0  24   0   7   7  10]\n",
      " [  6  28   1   0   3   0   0]\n",
      " [ 27   2 328   3   3   5   3]\n",
      " [  2   0   0  33   1   0   0]\n",
      " [  4   3   4   0 156   4   1]\n",
      " [  1   2   3   0   7  83   1]\n",
      " [  7   3   2   0   1   3 522]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f932561051df40638e3ba6084ffd34da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/26 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9065\n",
      "Recall: 0.9063\n",
      "Accuracy: 90.63%\n",
      "Confusion Matrix:\n",
      "[[303   0  26   0   8   2  10]\n",
      " [  6  28   1   0   3   0   0]\n",
      " [ 27   0 336   3   1   3   1]\n",
      " [  3   0   0  32   0   0   1]\n",
      " [  6   1   4   0 158   2   1]\n",
      " [  2   2   5   0   5  82   1]\n",
      " [ 13   3   3   0   3   4 512]]\n",
      "Model 1 Floating Point:\n",
      "  - Validation Loss: 0.0820\n",
      "  - Accuracy: 90.63%\n",
      "Model 1 Quantized:\n",
      "  - Validation Loss: 0.0850\n",
      "  - Accuracy: 90.63%\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60e65a628c7a426cac0e2dda62978a24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/26 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9123\n",
      "Recall: 0.9126\n",
      "Accuracy: 91.26%\n",
      "Confusion Matrix:\n",
      "[[303   0  23   0   8   1  14]\n",
      " [  6  28   1   0   3   0   0]\n",
      " [ 23   0 336   3   3   3   3]\n",
      " [  5   0   0  30   0   0   1]\n",
      " [  2   1   4   0 160   5   0]\n",
      " [  1   2   4   0   7  82   1]\n",
      " [  6   3   1   0   3   3 522]]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c73f5cdcbd49609a610e31389b9d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/26 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.9058\n",
      "Recall: 0.9057\n",
      "Accuracy: 90.57%\n",
      "Confusion Matrix:\n",
      "[[309   0  25   0   8   1   6]\n",
      " [  6  28   1   0   3   0   0]\n",
      " [ 22   0 345   0   2   2   0]\n",
      " [  3   0   2  26   2   0   3]\n",
      " [  2   1   5   0 162   2   0]\n",
      " [  3   2   5   0   6  80   1]\n",
      " [ 17   2   9   0   7   3 500]]\n",
      "Model 2 Floating Point:\n",
      "  - Validation Loss: 0.0727\n",
      "  - Accuracy: 91.26%\n",
      "Model 2 Quantized:\n",
      "  - Validation Loss: 0.0806\n",
      "  - Accuracy: 90.57%\n",
      "----------------------------------------\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "def model_size(mdl):\n",
    "    torch.save(mdl.state_dict(), \"tmp.pt\")\n",
    "    size = os.path.getsize(\"tmp.pt\") / 1e6\n",
    "    os.remove(\"tmp.pt\")\n",
    "    return f\"{size:.2f} MB\"\n",
    "\n",
    "# Quantize all models in models_fp\n",
    "model_quantized_static_list = [\n",
    "    quantize_model_and_calibrate(model_fp=mdl, calibration_loader=calibration_loader)\n",
    "    for mdl in models_fp\n",
    "]\n",
    "\n",
    "# Evaluate model sizes\n",
    "print(\"\\nModel Sizes\")\n",
    "print(\"=\" * 40)\n",
    "for i, (fp, q) in enumerate(zip(models_fp, model_quantized_static_list)):\n",
    "    print(f\"Model {i} size (floating point): {model_size(fp)}\")\n",
    "    print(f\"Model {i} size (quantized): {model_size(q)}\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Evaluate accuracy for all models\n",
    "print(\"\\nModel Evaluation Results\")\n",
    "print(\"=\" * 40)\n",
    "for i, (fp, q) in enumerate(zip(models_fp, model_quantized_static_list)):\n",
    "    val_loss_fp, accuracy_fp = validate_model(model=fp, val_loader=test_loader, criterion=criterion_type, device=device, metrics_flag=True)\n",
    "    val_loss_q, accuracy_q = validate_model(model=q, val_loader=test_loader, criterion=criterion_type, device=device, metrics_flag=True)\n",
    "    print(f\"Model {i} Floating Point:\\n  - Validation Loss: {val_loss_fp:.4f}\\n  - Accuracy: {accuracy_fp:.2f}%\")\n",
    "    print(f\"Model {i} Quantized:\\n  - Validation Loss: {val_loss_q:.4f}\\n  - Accuracy: {accuracy_q:.2f}%\")\n",
    "    print(\"-\" * 40)\n",
    "print(\"=\" * 40)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
